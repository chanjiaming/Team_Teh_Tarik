import os, re, math, statistics, shutil, json
import numpy as np
import pandas as pd
import joblib
from collections import defaultdict, Counter
from sklearn.model_selection import GroupKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier, _tree, export_text
from sklearn.preprocessing import QuantileTransformer, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, confusion_matrix

BASE_PROJECT_PATH = os.path.expanduser('~/Downloads/DramProject')
AI_TRAINING_PATH = os.path.join(BASE_PROJECT_PATH, 'AI_Training')
NEW_TRACE_PATH = os.path.join(BASE_PROJECT_PATH, 'New_Trace')
FREQ_MHZ = 1600.0
DEVICE_Gb = 16.0
CONFIGS = ['32ms', '48ms', '64ms']
FIT_PER_GB = 100.0 
EPS = 1e-30
COEFF = { "32ms": 1.0, "48ms": 2.2628, "64ms": 4.0395 }
GAMMAS = np.arange(0.1, 0.2, 0.025)
print(f" Project Base: {BASE_PROJECT_PATH}")

if os.path.exists(AI_TRAINING_PATH):
    shutil.rmtree(AI_TRAINING_PATH)
os.makedirs(AI_TRAINING_PATH)
for i in range(len(CONFIGS)):
    os.makedirs(os.path.join(AI_TRAINING_PATH, f"Scenario_{i+1}"), exist_ok=True)
ENERGY_RE = re.compile(r"Total Energy ->\s*([\d\.eE\-\+]+)")
LAT_RE    = re.compile(r"avg_read_latency_0:\s*([\d\.]+)")
CYC_RE    = re.compile(r"memory_system_cycles:\s*([\d\.]+)")
READ_RE   = re.compile(r"num_read_reqs_0:\s*([\d\.]+)|number_of_read_requests:\s*([\d\.]+)")
WRITE_RE  = re.compile(r"num_write_reqs_0:\s*([\d\.]+)|number_of_write_requests:\s*([\d\.]+)")
HITS_RE   = re.compile(r"row_hits_0:\s*([\d\.]+)|row_hits:\s*([\d\.]+)")
RMISS_RE  = re.compile(r"row_misses_0:\s*([\d\.]+)|row_misses:\s*([\d\.]+)")
RCONF_RE  = re.compile(r"row_conflicts_0:\s*([\d\.]+)|row_conflicts:\s*([\d\.]+)")
LLC_M_RE  = re.compile(r"llc_read_misses:\s*([\d\.]+)|cache_read_misses:\s*([\d\.]+)")
LLC_A_RE  = re.compile(r"llc_read_access:\s*([\d\.]+)|cache_read_access:\s*([\d\.]+)")

def safe_float(regex, text):
    m = regex.search(text)
    if m:
        for i in range(1, len(m.groups()) + 1):
            if m.group(i): return float(m.group(i))
    return 0.0

def geomean(values):
    vals = [v for v in values if v > 0]
    if not vals: return float('nan')
    return math.exp(sum(math.log(v) for v in vals) / len(vals))

def point_line_distance(px, py, ax, ay, bx, by):
    vx, vy = bx - ax, by - ay
    wx, wy = px - ax, py - ay
    denom = vx*vx + vy*vy
    if denom == 0: return math.hypot(px - ax, py - ay)
    return abs(vx*wy - vy*wx) / math.sqrt(denom)

def extract_threshold_values(tree, feature_names):
    """Extracts the exact split values for each feature."""
    tree_ = tree.tree_
    thresholds = defaultdict(list)
    for i in range(tree_.node_count):
        if tree_.feature[i] != _tree.TREE_UNDEFINED:
            feat_name = feature_names[tree_.feature[i]]
            threshold_val = tree_.threshold[i]
            thresholds[feat_name].append(threshold_val)

    return {k: sorted(list(set(v))) for k, v in thresholds.items()}

def extract_rules_from_tree(tree, feature_names):
    tree_ = tree.tree_
    feature_name = [
        feature_names[i] if i != _tree.TREE_UNDEFINED else "undefined!"
        for i in tree_.feature
    ]
    rules = []
    
    def recurse(node, path_rules):
        if tree_.feature[node] != _tree.TREE_UNDEFINED:
            name = feature_name[node]
            threshold = tree_.threshold[node]
            recurse(tree_.children_left[node], path_rules + [{"feature": name, "op": "<=", "value": threshold}])
            recurse(tree_.children_right[node], path_rules + [{"feature": name, "op": ">", "value": threshold}])
        else:
            class_idx = np.argmax(tree_.value[node])
            class_name = tree.classes_[class_idx]
            rules.append({"rules": path_rules, "prediction": class_name})
    recurse(0, [])
    return rules

def load_traces(path):
    aggregated = defaultdict(lambda: {cfg: [] for cfg in CONFIGS})
    if not os.path.exists(path): return aggregated
    for root, dirs, _ in os.walk(path):
        if any(c in d for c in CONFIGS for d in dirs):
            trace_name = os.path.basename(root)
            trace_key = trace_name.split('_')[0] if '_' in trace_name else trace_name
            for cfg in CONFIGS:
                try:
                    cfg_folder = next(d for d in os.listdir(root) if cfg in d)
                    full_path = os.path.join(root, cfg_folder)
                    dp_f = next(f for f in os.listdir(full_path) if 'drampower_report' in f)
                    ram_f = next(f for f in os.listdir(full_path) if 'ramulator2_report' in f)
                    with open(os.path.join(full_path, dp_f), 'r') as f: E = safe_float(ENERGY_RE, f.read())
                    with open(os.path.join(full_path, ram_f), 'r') as f:
                        txt = f.read()
                        lat_c, tot_c = safe_float(LAT_RE, txt), safe_float(CYC_RE, txt)
                        n_r, n_w = safe_float(READ_RE, txt), safe_float(WRITE_RE, txt)
                        rh, rm, rc = safe_float(HITS_RE, txt), safe_float(RMISS_RE, txt), safe_float(RCONF_RE, txt)
                        lm, la = safe_float(LLC_M_RE, txt), safe_float(LLC_A_RE, txt)
                    lat_s = lat_c / (FREQ_MHZ * 1e6)
                    dur_h = (tot_c / (FREQ_MHZ * 1e6)) / 3600.0
                    M = E * (lat_s ** 2)
                    SER = 1.0 - math.exp(-((FIT_PER_GB / 1e9) * DEVICE_Gb * dur_h))
                    denom = rh + rm + rc
                    aggregated[trace_key][cfg].append({
                        "M": M, "SER": SER,
                        "Incoming_Req_Per_Cycle": (n_r + n_w) / tot_c,
                        "Read_Intensity": n_r / (n_r + n_w) if (n_r+n_w)>0 else 0,
                        "RB_Locality": rh / denom if denom > 0 else 0,
                        "RB_Conflict_Rate": rc / denom if denom > 0 else 0,
                        "LLC_Miss_Rate": lm / la if la > 0 else 0
                    })
                except: continue
    return aggregated

print(" Loading Training Traces...")
agg_train = load_traces(BASE_PROJECT_PATH)
data_train = defaultdict(dict)
for t, cfgs in agg_train.items():
    for cfg, runs in cfgs.items():
        if runs:
            data_train[t][cfg] = {
                "M": statistics.mean(r["M"] for r in runs),
                "SER": statistics.mean(r["SER"] for r in runs),
                "runs": runs
            }
valid_train_traces = sorted([t for t in data_train if all(cfg in data_train[t] for cfg in CONFIGS)])
print(f" Loaded {len(valid_train_traces)} valid training traces.")
print("  Running Pareto-Knee Gamma Sweep...")
gamma_results = []
per_gamma_selections = {}
for gamma in GAMMAS:
    M_norms, ratio_norms = [], []
    selections = {}
    for t in valid_train_traces:
        M32 = data_train[t]["32ms"]["M"]
        SER32 = max(data_train[t]["32ms"]["SER"], EPS)
        best_cfg, best_score, best_M, best_ratio = None, float("inf"), 0, 0
        for cfg in CONFIGS:
            M = data_train[t][cfg]["M"]
            SER = max(data_train[t][cfg]["SER"], EPS)
            ratio = SER / SER32
            score = M * ratio * (COEFF[cfg] ** gamma)
            if score < best_score:
                best_score, best_cfg, best_M, best_ratio = score, cfg, M, ratio
        selections[t] = {"cfg": best_cfg, "M": best_M, "ratio": best_ratio}
        M_norms.append(best_M / M32)
        ratio_norms.append(best_ratio)
    gamma_results.append({
        "gamma": gamma,
        "M_impr": 1.0 - geomean(M_norms),
        "rel_deg_gm": geomean(ratio_norms)
    })
    per_gamma_selections[gamma] = selections
g_sorted = sorted(gamma_results, key=lambda d: d["gamma"])
A, B = g_sorted[0], g_sorted[-1]
best_gamma = max(g_sorted, key=lambda r: point_line_distance(
    r["rel_deg_gm"], r["M_impr"], A["rel_deg_gm"], A["M_impr"], B["rel_deg_gm"], B["M_impr"]
))["gamma"]
final_sel = per_gamma_selections[best_gamma]
print(f"\n OPTIMAL GAMMA: {best_gamma:.3f}")
print("\n=== Final t_REFI Selection per Trace (Training Data) ===")
print(f"{'Trace Name':<30} | {'Selected':<8} | {'M/M32':<8} | {'SER Ratio':<8}")
print("-" * 65)
for t in valid_train_traces:
    sel = final_sel[t]
    m_norm = sel["M"] / data_train[t]["32ms"]["M"]
    print(f"{t:<30} | {sel['cfg']:<8} | {m_norm:.4f}   | {sel['ratio']:.4f}")
print("-" * 65 + "\n")
print(" Preparing Training Data...")
train_rows = []
for t in valid_train_traces:
    winner = final_sel[t]["cfg"]
    for run in data_train[t][winner]["runs"]:
        train_rows.append({
            'Incoming_Req_Per_Cycle': run['Incoming_Req_Per_Cycle'],
            'Read_Intensity': run['Read_Intensity'],
            'RB_Locality': run['RB_Locality'],
            'RB_Conflict_Rate': run['RB_Conflict_Rate'],
            'LLC_Miss_Rate': run['LLC_Miss_Rate'],
            'Traffic_Risk': run['Incoming_Req_Per_Cycle'] * (1.0 - run['RB_Locality']),
            'Conflict_Load': run['RB_Conflict_Rate'] * run['Read_Intensity'],
            'Label': winner
        })

df_train = pd.DataFrame(train_rows)
X = df_train.drop(columns=['Label'])
y = df_train['Label']
print("\n Training TOP LEVEL Model")
clf1 = RandomForestClassifier(n_estimators=100, class_weight='balanced_subsample', random_state=42)
clf2 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)
clf3 = ExtraTreesClassifier(n_estimators=100, class_weight='balanced_subsample', random_state=42)
voting_clf = VotingClassifier(estimators=[('rf', clf1), ('gb', clf2), ('et', clf3)], voting='soft')
top_level_model = Pipeline([
    ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),
    ('scaler', QuantileTransformer(output_distribution='normal', random_state=42)),
    ('ensemble', voting_clf)
])
top_level_model.fit(X, y)
print("\n Tuning Top Level Safety Margin...")
probs = top_level_model.predict_proba(X)
classes = list(voting_clf.classes_)
idx_64, idx_48 = classes.index('64ms'), classes.index('48ms')
best_margin, best_acc = 0.0, 0.0
for margin in np.arange(0.0, 0.21, 0.01):
    temp_preds = []
    for p in probs:
        top = np.argmax(p)
        label = classes[top]
        if label == '64ms' and (p[idx_64] - p[idx_48]) < margin:
            temp_preds.append('48ms')
        else:
            temp_preds.append(label)
    cm_temp = confusion_matrix(y, temp_preds, labels=CONFIGS)
    r_48_64 = cm_temp[1][2]
    r_32_64 = cm_temp[0][2]
    r_32_48 = cm_temp[0][1]
    total_risk = r_48_64 + r_32_64 + r_32_48
    acc = accuracy_score(y, temp_preds)
    
    if total_risk == 0:
        if acc >= best_acc:
            best_acc = acc
            best_margin = margin
print(f" Top Level Margin: {best_margin*100:.1f}%")
top_level_preds_safe = []
for p in probs:
    top = np.argmax(p)
    label = classes[top]
    if label == '64ms' and (p[idx_64] - p[idx_48]) < best_margin:
        top_level_preds_safe.append('48ms')
    else:
        top_level_preds_safe.append(label)
print("\n" + "="*40 + "\n TOP LEVEL TRAINING CONFUSION MATRIX \n" + "="*40)
print(f"Top Level Accuracy (Safe): {accuracy_score(y, top_level_preds_safe)*100:.2f}%")
cm_top = confusion_matrix(y, top_level_preds_safe, labels=CONFIGS)
print(f"{'True \\ Pred':<12}", end=""); [print(f"{c:<8}", end="") for c in CONFIGS]; print()
for i, row in enumerate(cm_top):
    print(f"{CONFIGS[i]:<12}", end=""); [print(f"{val:<8}", end="") for val in row]; print()
print("\n Transferring to BASE LEVEL Tree")
top_level_probs = top_level_model.predict_proba(X)
y_safe_labels = []
for p in top_level_probs:
    top = np.argmax(p)
    label = classes[top]
    if label == '64ms' and (p[idx_64] - p[idx_48]) < best_margin:
        y_safe_labels.append('48ms')
    else:
        y_safe_labels.append(label)
base_level_tree = DecisionTreeClassifier(max_depth=6, random_state=42, class_weight='balanced')
base_level_tree.fit(X, y_safe_labels)
train_preds_base = base_level_tree.predict(X)
print(f"\nBase Level Tree Depth: {base_level_tree.get_depth()}")
print(f"Base Level Training Accuracy (vs True Original Labels): {accuracy_score(y, train_preds_base)*100:.2f}%")
print("\n" + "="*40 + "\n  BASE LEVEL TRAINING CONFUSION MATRIX \n" + "="*40)
cm_train_base = confusion_matrix(y, train_preds_base, labels=CONFIGS)
print(f"{'True \\ Pred':<12}", end=""); [print(f"{c:<8}", end="") for c in CONFIGS]; print()
for i, row in enumerate(cm_train_base):
    print(f"{CONFIGS[i]:<12}", end=""); [print(f"{val:<8}", end="") for val in row]; print()
print("\n" + "="*40 + "\n  BASE LEVEL TREE STRUCTURE (Threshold Nodes) \n" + "="*40)
tree_rules = export_text(base_level_tree, feature_names=list(X.columns))
print(tree_rules)
print("\n Extracting Hardware Thresholds from Base Level...")
rules = extract_rules_from_tree(base_level_tree, X.columns)
json_path = os.path.join(AI_TRAINING_PATH, "hardware_rules.json")
with open(json_path, "w") as f:
    json.dump(rules, f, indent=2)
csv_rows = []
for i, r in enumerate(rules):
    pred = r['prediction']
    rule_str = " AND ".join([f"{c['feature']} {c['op']} {c['value']:.4f}" for c in r['rules']])
    csv_rows.append({"Rule_ID": i+1, "Prediction": pred, "Logic": rule_str})
pd.DataFrame(csv_rows).to_csv(os.path.join(AI_TRAINING_PATH, "hardware_logic.csv"), index=False)
thresholds = extract_threshold_values(base_level_tree, X.columns)
print("\n" + "="*40 + "\n  HARDWARE PARAMETER LIST (Threshold Values) \n" + "="*40)
print(f"{'Feature':<25} | {'Count':<5} | {'Cutoff Values'}")
print("-" * 65)
for feat, vals in thresholds.items():
    val_str = ", ".join([f"{v:.4f}" for v in vals])
    print(f"{feat:<25} | {len(vals):<5} | {val_str}")
thresh_rows = []
for feat, vals in thresholds.items():
    for v in vals:
        thresh_rows.append({"Feature": feat, "Threshold": v})
pd.DataFrame(thresh_rows).to_csv(os.path.join(AI_TRAINING_PATH, "hardware_thresholds_flat.csv"), index=False)
print(f"Saved: {json_path}")
print("\n" + "="*40 + "\n  VALIDATION PHASE (Using Base Level Tree) \n" + "="*40)
agg_val = load_traces(NEW_TRACE_PATH)

if not agg_val:
    print(" No traces found.")
else:
    val_rows = []
    for t, cfgs in agg_val.items():
        if not all(c in cfgs for c in CONFIGS): continue
        M32 = statistics.mean(r["M"] for r in cfgs["32ms"])
        SER32 = max(statistics.mean(r["SER"] for r in cfgs["32ms"]), EPS)
        best_c, best_s = None, float("inf")
        for cfg in CONFIGS:
            M = statistics.mean(r["M"] for r in cfgs[cfg])
            SER = max(statistics.mean(r["SER"] for r in cfgs[cfg]), EPS)
            score = M * (SER/SER32) * (COEFF[cfg] ** best_gamma)
            if score < best_s: best_s, best_c = score, cfg
            
        for run in cfgs[best_c]:
            val_rows.append({
                'Incoming_Req_Per_Cycle': run['Incoming_Req_Per_Cycle'],
                'Read_Intensity': run['Read_Intensity'],
                'RB_Locality': run['RB_Locality'],
                'RB_Conflict_Rate': run['RB_Conflict_Rate'],
                'LLC_Miss_Rate': run['LLC_Miss_Rate'],
                'Traffic_Risk': run['Incoming_Req_Per_Cycle'] * (1.0 - run['RB_Locality']),
                'Conflict_Load': run['RB_Conflict_Rate'] * run['Read_Intensity'],
                'True_Label': best_c
            })

    if val_rows:
        df_val = pd.DataFrame(val_rows)
        X_val = df_val.drop(columns=['True_Label'])
        y_val = df_val['True_Label']
        preds_base = base_level_tree.predict(X_val)
        print(f"\n Base Level Validation Accuracy: {accuracy_score(y_val, preds_base)*100:.2f}%")  
        print("\n VALIDATION CONFUSION MATRIX (Hardware Logic):")
        cm = confusion_matrix(y_val, preds_base, labels=CONFIGS)
        print(f"{'True \\ Pred':<12}", end=""); [print(f"{c:<8}", end="") for c in CONFIGS]; print()
        for i, row in enumerate(cm):
            print(f"{CONFIGS[i]:<12}", end=""); [print(f"{val:<8}", end="") for val in row]; print()
    else:
        print(" No valid traces found.")
print("\n Done.")